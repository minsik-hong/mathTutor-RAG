import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from models.inference import get_lowest_prob_problems


# âœ… API í‚¤ ë¡œë”©
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()
CHROMA_DB_DIR = "db/chroma_math_json"

# âœ… ë²¡í„° DB ë¡œë“œ
def load_vector_db(persist_dir=CHROMA_DB_DIR):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    return Chroma(persist_directory=persist_dir, embedding_function=embeddings)

# âœ… json ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì§ˆë¬¸
def ask_question_by_chapter_id(vectordb, chapter_id, question):
    retriever = vectordb.as_retriever(
        # k=1 -> ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 1ê°œë§Œ ê²€ìƒ‰
        search_kwargs={"k": 1, "filter": {"chapter_id": str(chapter_id)}} # ì´ ê³³ì— ì°¸ì¡° í‚¤ ì…ë ¥
    )

    prompt_template = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™, ì¤‘í•™êµ ìˆ˜í•™ êµê³¼ ê³¼ì •ì„ ì˜ ì•„ëŠ” êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì„œëŠ” idê°€ {chapter_id}ì¸ ê°œë…ì— ëŒ€í•œ ì„¤ëª…, í•™ê¸° ì •ë³´, ê´€ë ¨ ë‹¨ì›, ì„±ì·¨ ê¸°ì¤€ ë“±ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

í•™ìƒì´ ì´ ê°œë…ì„ ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬:
1. ê°œë…ì„ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê³ ,
2. ì´ ê°œë…ì´ í¬í•¨ëœ ë‹¨ì›ê³¼ ëª©ì°¨ íë¦„ ì†ì—ì„œ ì–´ë–¤ ìœ„ì¹˜ì¸ì§€ ì†Œê°œí•˜ë©°,
3. ì„±ì·¨ ê¸°ì¤€ì´ ì˜ë¯¸í•˜ëŠ” ë‚´ìš©ì„ í’€ì–´ ì„¤ëª…í•˜ê³ ,
4. í•™ìƒì´ ì—°ìŠµí•´ë³¼ ìˆ˜ ìˆëŠ” ì‹¤ìƒí™œ ê¸°ë°˜ì˜ ë¬¸ì œë¥¼ í•œë‘ ê°œ ì œì‹œí•˜ê³ ,
5. ë¬¸ì œì— ëŒ€í•œ ìˆ˜ì‹ ì‘ì„± ì‹œ **LaTeX ìˆ˜ì‹**ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ìˆ˜ì‹ì€ `$...$` ë˜ëŠ” `$$...$$` ë¡œ ê°ì‹¸ ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”:

### ğŸ§  ê°œë… ì„¤ëª…
...

### ğŸ“˜ ë‹¨ì›ê³¼ ëª©ì°¨ ì •ë³´
...

### ğŸ¯ ì„±ì·¨ ê¸°ì¤€ í•´ì„¤
...

### ğŸ§© ì—°ìŠµ ë¬¸ì œ
ë¬¸ì œ 1: ...
ë¬¸ì œ 2: ...

### âœ… ì •ë‹µ ë° í•´ì„¤
ì •ë‹µ 1: ...
ì •ë‹µ 2: ...
""".replace("{chapter_id}", str(chapter_id))

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3, openai_api_key=OPENAI_API_KEY),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": question})
    answer = result["result"]

    # ğŸ” ê´€ë ¨ ë©”íƒ€ë°ì´í„°ì—ì„œ achievement_name ì¶”ì¶œ
    source_doc = result["source_documents"][0]
    achievement_name = source_doc.metadata.get("achievement_name", "ì •ë³´ ì—†ìŒ")

    return answer, achievement_name

# achievement_name ê¸°ë°˜ ë¬¸ì œ ì°¾ê¸°
def ask_question_by_achievement_name(vectordb, achievement_name, question):
    retriever = vectordb.as_retriever(
        search_kwargs={
            "k": 1,  # ê°€ì¥ ìœ ì‚¬í•œ í•˜ë‚˜ì˜ ë¬¸ì„œë§Œ ê°€ì ¸ì˜´
        }
    )

    # ê²€ìƒ‰ ì§ˆì˜ ìì²´ì— achievement_nameì„ í¬í•¨ì‹œì¼œ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ìœ ë„
    full_query = f"{achievement_name} ê´€ë ¨ ìˆ˜í•™ ë¬¸ì œì™€ í•´ì„¤: {question}"

    prompt_template = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ë° ì¤‘í•™êµ ìˆ˜í•™ ë¬¸ì œë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” êµì‚¬ì…ë‹ˆë‹¤.

ì•„ë˜ ë¬¸ì„œëŠ” íŠ¹ì • ì„±ì·¨ê¸°ì¤€ì— í•´ë‹¹í•˜ëŠ” ìˆ˜í•™ ë¬¸ì œì…ë‹ˆë‹¤.

í•™ìƒì´ ì´ ë¬¸ì œë¥¼ ì´í•´í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬:
1. ë¬¸ì œë¥¼ ì‰½ê²Œ í’€ì–´ì£¼ëŠ” í•´ì„¤ì„ ì œê³µí•˜ê³ ,
2. í•„ìš”í•œ ê°œë… ì„¤ëª…ì„ í•¨ê»˜ í•´ì£¼ê³ ,
3. ì •ë‹µê³¼ ì´ìœ ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
4. ë¬¸ì œì— ëŒ€í•œ ìˆ˜ì‹ ì‘ì„± ì‹œ **LaTeX ìˆ˜ì‹**ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ìˆ˜ì‹ì€ `$...$` ë˜ëŠ” `$$...$$` ë¡œ ê°ì‹¸ ì£¼ì„¸ìš”.

ë¬¸ì œ:
{context}

ì§ˆë¬¸:
{question}
"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3, openai_api_key=OPENAI_API_KEY),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": question})
    answer = result["result"]

    # source ë¬¸ì„œì—ì„œ ë¬¸ì œ ì´ë¯¸ì§€ ë° ì£¼ì œ ê°€ì ¸ì˜¤ê¸°
    source_doc = result["source_documents"][0]
    image_path = source_doc.metadata.get("image_path")
    question_topic = source_doc.metadata.get("question_topic")

    # ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ë‹µë³€ì— í¬í•¨ (ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜)
    if image_path:
        # Markdownì´ ì €ì¥ë  genResult í´ë” ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
        rel_image_path = os.path.relpath(image_path, start="genResult")
        answer += f"\n\n**ë¬¸ì œ ì´ë¯¸ì§€**:\n![ë¬¸ì œ ì´ë¯¸ì§€]({rel_image_path})"


    # ë¬¸ì œ ì£¼ì œë„ í•¨ê»˜ ì œê³µ
    if question_topic:
        answer = f"### ë¬¸ì œ ì£¼ì œ: {question_topic}\n\n" + answer

    return answer

# âœ… Markdown ì €ì¥
def save_answer_as_markdown(question, answer, file_path="result.md", folder="genResult"):
    os.makedirs(folder, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    full_path = os.path.join(folder, file_path)
    with open(full_path, "w", encoding="utf-8") as f:
        f.write(f"# ì§ˆë¬¸\n\n{question}\n\n")
        f.write(f"# ë‹µë³€\n\n{answer}\n")
    print(f"ë‹µë³€ì´ '{full_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    vectordb = load_vector_db()
    print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")

    # ğŸ”¹ Inferenceì—ì„œ Problem ID ê°€ì ¸ì˜¤ê¸°
    model_path = "/Users/hongminsik/Desktop/mathRag/models/model_best.pth"
    csv_path = "/Users/hongminsik/Desktop/mathRag/i-scream/i-scream_test.csv"
    student_index = 1  # ë¶„ì„í•  í•™ìƒì˜ ì¸ë±ìŠ¤
    num_problems = 1865  # ì´ ë¬¸ì œ ìˆ˜


    # Get Problem IDs with the lowest probabilities
    problem_ids = get_lowest_prob_problems(model_path, csv_path, student_index, num_problems, top_n=1) # ê°œìˆ˜ ì¡°ì •
    print(f"ğŸ”¹ ê°€ì ¸ì˜¨ Problem IDs: {problem_ids}")

    # ğŸ”¹ ê° Problem IDì— ëŒ€í•´ ì§ˆë¬¸ ìƒì„± ë° ì €ì¥
    for problem_id in problem_ids:
        print(f"\nProcessing Problem ID: {problem_id}")

        # 1ï¸âƒ£ ê°œë… ì„¤ëª… ë° achievement_name ì¶”ì¶œ
        concept_question = f"í•™ìƒì´ ê°œë… id {problem_id}ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ê°œë…ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ê³ , ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        concept_answer, achievement_name = ask_question_by_chapter_id(vectordb, problem_id, concept_question)
        print(f"ì¶”ì¶œëœ ì„±ì·¨ ê¸°ì¤€ ì´ë¦„: {achievement_name}")

        # 2ï¸âƒ£ ì„±ì·¨ ê¸°ì¤€ ê¸°ë°˜ ë¬¸ì œ ì˜ˆì‹œ ë° í•´ì„¤ ìƒì„±
        if achievement_name != "ì •ë³´ ì—†ìŒ":
            problem_question = f"í•™ìƒì´ '{achievement_name}' ì„±ì·¨ê¸°ì¤€ì— ëŒ€í•œ ë¬¸ì œë¥¼ ì´í•´í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¬¸ì œ í•´ì„¤ê³¼ ê°œë… ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”."
            problem_answer = ask_question_by_achievement_name(vectordb, achievement_name, problem_question)
        else:
            problem_answer = "ê´€ë ¨ëœ ì„±ì·¨ ê¸°ì¤€ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¬¸ì œ ì˜ˆì‹œë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 3ï¸âƒ£ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥
        output_file = f"result_concept_plus_problem_{problem_id}.md"
        os.makedirs("genResult", exist_ok=True)
        full_path = os.path.join("genResult", output_file)

        with open(full_path, "w", encoding="utf-8") as f:
            f.write(f"# ì§ˆë¬¸ (ê°œë…)\n\n{concept_question}\n\n")
            f.write(f"# ê°œë… ê´€ë ¨ ë‹µë³€\n\n{concept_answer}\n\n")
            f.write(f"# ì§ˆë¬¸ (ë¬¸ì œ ì˜ˆì‹œ)\n\n{problem_question}\n\n")
            f.write(f"# ë¬¸ì œ ì˜ˆì‹œ ë° í•´ì„¤\n\n{problem_answer}\n")

        print(f"âœ… Problem ID {problem_id}ì— ëŒ€í•œ ê°œë… + ë¬¸ì œ ì„¤ëª… ì €ì¥ ì™„ë£Œ: {output_file}")

