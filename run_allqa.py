# run_jsonqa.py
import os
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Load API key
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()

CHROMA_DB_DIR = "db/chroma_math_json"

def load_vector_db(persist_dir=CHROMA_DB_DIR):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    return Chroma(persist_directory=persist_dir, embedding_function=embeddings)

def ask_question_by_id(vectordb, concept_id, question):
    retriever = vectordb.as_retriever(
        search_kwargs={"k": 1, "filter": {"concept_id": str(concept_id)}}
    )

    prompt_template = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ìˆ˜í•™ êµê³¼ ê³¼ì •ì„ ì˜ ì•„ëŠ” êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì„œëŠ” idê°€ {concept_id}ì¸ ê°œë…ì— ëŒ€í•œ ì„¤ëª…, í•™ê¸° ì •ë³´, ê´€ë ¨ ë‹¨ì›, ì„±ì·¨ ê¸°ì¤€ ë“±ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

í•™ìƒì´ ì´ ê°œë…ì„ ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬:
1. ê°œë…ì„ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê³ ,
2. ì´ ê°œë…ì´ í¬í•¨ëœ ë‹¨ì›ê³¼ ëª©ì°¨ íë¦„ ì†ì—ì„œ ì–´ë–¤ ìœ„ì¹˜ì¸ì§€ ì†Œê°œí•˜ë©°,
3. ì„±ì·¨ ê¸°ì¤€ì´ ì˜ë¯¸í•˜ëŠ” ë‚´ìš©ì„ í’€ì–´ ì„¤ëª…í•˜ê³ ,
4. í•™ìƒì´ ì—°ìŠµí•´ë³¼ ìˆ˜ ìˆëŠ” ì‹¤ìƒí™œ ê¸°ë°˜ì˜ ë¬¸ì œë¥¼ í•œë‘ ê°œ ì œì‹œí•˜ê³ ,
5. LaTeX ìˆ˜ì‹ì´ í¬í•¨ëœ í•´ì„¤ì„ ì œê³µí•©ë‹ˆë‹¤.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”:

### ğŸ§  ê°œë… ì„¤ëª…
...

### ğŸ“˜ ë‹¨ì›ê³¼ ëª©ì°¨ ì •ë³´
...

### ğŸ¯ ì„±ì·¨ ê¸°ì¤€ í•´ì„¤
...

### ğŸ§© ì—°ìŠµ ë¬¸ì œ
ë¬¸ì œ 1: ...
ë¬¸ì œ 2: ...

### âœ… ì •ë‹µ ë° í•´ì„¤
ì •ë‹µ 1: ...
ì •ë‹µ 2: ...
""".replace("{concept_id}", str(concept_id))  # âœ… concept_idë§Œ Pythonì—ì„œ ì§ì ‘ ì‚½ì…

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(
            model_name="gpt-3.5-turbo-16k",
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY
        ),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": question})
    return result["result"]

def save_answer_as_markdown(question, answer, file_path="result.md"):
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(f"# ğŸ“Œ ì§ˆë¬¸\n\n{question}\n\n")
        f.write(f"# ğŸ’¡ ë‹µë³€\n\n{answer}\n")
    print(f"ğŸ“„ ë‹µë³€ì´ '{file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    vectordb = load_vector_db()
    print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")

    concept_id = 5844
    question = "í•™ìƒì´ ê°œë… id 5844ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ê°œë…ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ê³ , ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."

    answer = ask_question_by_id(vectordb, concept_id, question)
    print("ğŸ“Œ ì§ˆë¬¸:", question)
    print("ğŸ’¡ ë‹µë³€:", answer)
    save_answer_as_markdown(question, answer)