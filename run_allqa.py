import os
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from models.inference import get_lowest_prob_problems


# âœ… API í‚¤ ë¡œë”©
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()
CHROMA_DB_DIR = "db/chroma_math_json"

# âœ… ë²¡í„° DB ë¡œë“œ
def load_vector_db(persist_dir=CHROMA_DB_DIR):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    return Chroma(persist_directory=persist_dir, embedding_function=embeddings)

# âœ… chapter_id ê¸°ë°˜ ì§ˆë¬¸
def ask_question_by_chapter_id(vectordb, chapter_id, question):
    retriever = vectordb.as_retriever(
        search_kwargs={"k": 1, "filter": {"chapter_id": str(chapter_id)}}
    )

    prompt_template = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ìˆ˜í•™ êµê³¼ ê³¼ì •ì„ ì˜ ì•„ëŠ” êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì„œëŠ” idê°€ {chapter_id}ì¸ ê°œë…ì— ëŒ€í•œ ì„¤ëª…, í•™ê¸° ì •ë³´, ê´€ë ¨ ë‹¨ì›, ì„±ì·¨ ê¸°ì¤€ ë“±ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

í•™ìƒì´ ì´ ê°œë…ì„ ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬:
1. ê°œë…ì„ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê³ ,
2. ì´ ê°œë…ì´ í¬í•¨ëœ ë‹¨ì›ê³¼ ëª©ì°¨ íë¦„ ì†ì—ì„œ ì–´ë–¤ ìœ„ì¹˜ì¸ì§€ ì†Œê°œí•˜ë©°,
3. ì„±ì·¨ ê¸°ì¤€ì´ ì˜ë¯¸í•˜ëŠ” ë‚´ìš©ì„ í’€ì–´ ì„¤ëª…í•˜ê³ ,
4. í•™ìƒì´ ì—°ìŠµí•´ë³¼ ìˆ˜ ìˆëŠ” ì‹¤ìƒí™œ ê¸°ë°˜ì˜ ë¬¸ì œë¥¼ í•œë‘ ê°œ ì œì‹œí•˜ê³ ,
5. LaTeX ìˆ˜ì‹ì´ í¬í•¨ëœ í•´ì„¤ì„ ì œê³µí•©ë‹ˆë‹¤.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”:

### ğŸ§  ê°œë… ì„¤ëª…
...

### ğŸ“˜ ë‹¨ì›ê³¼ ëª©ì°¨ ì •ë³´
...

### ğŸ¯ ì„±ì·¨ ê¸°ì¤€ í•´ì„¤
...

### ğŸ§© ì—°ìŠµ ë¬¸ì œ
ë¬¸ì œ 1: ...
ë¬¸ì œ 2: ...

### âœ… ì •ë‹µ ë° í•´ì„¤
ì •ë‹µ 1: ...
ì •ë‹µ 2: ...
""".replace("{chapter_id}", str(chapter_id))

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model_name="gpt-3.5-turbo-16k", temperature=0.3, openai_api_key=OPENAI_API_KEY),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": question})
    return result["result"]

# âœ… ë¬¸ì œ ID ê¸°ë°˜ ì§ˆë¬¸ (ë¬¸ì œ IDëŠ” ë¬¸ìì—´)
def ask_question_by_problem_id(vectordb, problem_id, question):
    retriever = vectordb.as_retriever(
        search_kwargs={"k": 1, "filter": {"id": str(problem_id)}}
    )

    prompt_template = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ë° ì¤‘í•™êµ ìˆ˜í•™ ë¬¸ì œë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” êµì‚¬ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì„œëŠ” idê°€ {problem_id}ì¸ ì‹¤ì œ ìˆ˜í•™ ë¬¸ì œì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì„¤ëª…ì…ë‹ˆë‹¤.

í•™ìƒì´ ë¬¸ì œë¥¼ ì´í•´í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬:
1. ë¬¸ì œë¥¼ ì‰½ê²Œ í’€ì–´ì£¼ëŠ” í•´ì„¤ì„ ì œê³µí•˜ê³ ,
2. í•„ìš”í•œ ê°œë… ì„¤ëª…ì„ í•¨ê»˜ í•´ì£¼ê³ ,
3. ì •ë‹µê³¼ ì´ìœ ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

ë¬¸ì œ:
{context}

ì§ˆë¬¸:
{question}
""".replace("{problem_id}", str(problem_id))

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3, openai_api_key=OPENAI_API_KEY),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": question})
    answer = result["result"]

    # âœ… ì´ë¯¸ì§€ê°€ ìˆì„ ê²½ìš° í¬í•¨
    source_doc = result["source_documents"][0]
    image_path = source_doc.metadata.get("image_path")
    if image_path:
        answer += f"\n\n![ë¬¸ì œ ì´ë¯¸ì§€]({image_path})"

    return answer

# âœ… Markdown ì €ì¥
def save_answer_as_markdown(question, answer, file_path="result.md"):
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(f"# ğŸ“Œ ì§ˆë¬¸\n\n{question}\n\n")
        f.write(f"# ğŸ’¡ ë‹µë³€\n\n{answer}\n")
    print(f"ğŸ“„ ë‹µë³€ì´ '{file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    vectordb = load_vector_db()
    print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")

    # ğŸ”¹ Inferenceì—ì„œ Problem ID ê°€ì ¸ì˜¤ê¸°
    model_path = "/Users/hongminsik/Desktop/mathRag/models/model_best.pth"
    csv_path = "/Users/hongminsik/Desktop/mathRag/i-scream/i-scream_test.csv"
    student_index = 1  # ë¶„ì„í•  í•™ìƒì˜ ì¸ë±ìŠ¤
    num_problems = 1865  # ì´ ë¬¸ì œ ìˆ˜


    # Get Problem IDs with the lowest probabilities
    problem_ids = get_lowest_prob_problems(model_path, csv_path, student_index, num_problems, top_n=10)
    print(f"ğŸ”¹ ê°€ì ¸ì˜¨ Problem IDs: {problem_ids}")

    # ğŸ”¹ ê° Problem IDì— ëŒ€í•´ ì§ˆë¬¸ ìƒì„± ë° ì €ì¥
    for problem_id in problem_ids:
        question = f"í•™ìƒì´ ê°œë… id {problem_id}ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ê°œë…ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ê³ , ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        answer = ask_question_by_chapter_id(vectordb, problem_id, question)
        output_file = f"result_concept_{problem_id}.md"
        save_answer_as_markdown(question, answer, output_file)
        print(f"âœ… Problem ID {problem_id}ì— ëŒ€í•œ ë‹µë³€ ì €ì¥ ì™„ë£Œ: {output_file}")

    # # ğŸ”¹ chapter_id = 528
    # chapter_id = 414
    # question1 = "í•™ìƒì´ ê°œë… id 2773ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ê°œë…ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ê³ , ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
    # answer1 = ask_question_by_chapter_id(vectordb, chapter_id, question1)
    # save_answer_as_markdown(question1, answer1, "result_concept_2773.md")

    # # ğŸ”¹ ë¬¸ì œ ID = "25763_84170"
    # problem_id = "25763_84170"
    # question2 = "ë¬¸ì œ ID 25763_84170ë²ˆì„ í•™ìƒì´ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¬¸ì œ í•´ì„¤ê³¼ ê°œë… ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”."
    # answer2 = ask_question_by_problem_id(vectordb, problem_id, question2)
    # save_answer_as_markdown(question2, answer2, "result_problem_25763_84170.md")
