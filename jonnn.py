import os
from dotenv import load_dotenv

# âœ… ìµœì‹  import ê²½ë¡œ ë°˜ì˜
from langchain_community.document_loaders import JSONLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Load API key
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()

# JSON íŒŒì¼ ê²½ë¡œ
JSON_FILE = "data/P5_1_01_00042_30429.json"
CHROMA_DB_DIR = "db/chroma_math_json"

# 1. JSON ë¬¸ì„œ ë¡œë“œ
def load_json(path):
    jq_schema = '''
    . as $item
    | {
        page_content: ($item.OCR_info[0].question_text),
        metadata: {
            id: $item.id,
            grade: $item.question_info[0].question_grade,
            unit: $item.question_info[0].question_unit,
            topic_name: $item.question_info[0].question_topic_name,
            difficulty: $item.question_info[0].question_difficulty
        }
    }
    '''
    loader = JSONLoader(
        file_path=path,
        jq_schema=jq_schema,
        text_content=False
    )
    return loader.load()

# 2. ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì €ì¥
def save_to_vector_db(docs, persist_dir=CHROMA_DB_DIR):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = Chroma.from_documents(docs, embedding=embeddings, persist_directory=persist_dir)
    vectordb.persist()
    return vectordb

# 3. ì €ì¥ëœ ë²¡í„° DB ë¡œë“œ
def load_vector_db(persist_dir=CHROMA_DB_DIR):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    return vectordb

# 4. ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°
def ask_question(vectordb, question):
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    # âœ… PromptTemplateì— context í¬í•¨
    prompt_template = """
ë‹¹ì‹ ì€ ìˆ˜í•™ êµìœ¡ì„ ìœ„í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìƒì—ê²Œ ì ì ˆí•œ ìˆ˜í•™ ë¬¸ì œë¥¼ ìƒì„±í•˜ê³ , ëª…í™•í•œ ì •ë‹µì„ LaTeX í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:

ë¬¸ì œ:
...

ë‹µ:
(LaTeX ìˆ˜ì‹ í¬í•¨)
"""

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(
            model_name="gpt-3.5-turbo-16k",
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY
        ),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain({"query": question})
    return result["result"]

# 5. ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥
def save_answer_as_markdown(question, answer, file_path="result.md"):
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(f"# ğŸ“Œ ì§ˆë¬¸\n\n{question}\n\n")
        f.write(f"# ğŸ’¡ ë‹µë³€\n\n{answer}\n")
    print(f"ğŸ“„ ë‹µë³€ì´ '{file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    # Step 1: JSON ë¡œë“œ
    documents = load_json(JSON_FILE)
    print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ì¡°ê°ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # Step 2: ë²¡í„° DBì— ì €ì¥
    vectordb = save_to_vector_db(documents)
    print("âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ")

    # Step 3: ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
    question = "ê³±ì…ˆê³¼ ë‚˜ëˆ—ì…ˆì´ ì„ì—¬ ìˆëŠ” ì‹ì˜ ê³„ì‚°ë¬¸ì œ ë‚´ì¤˜"
    answer = ask_question(vectordb, question)

    # Step 4: ì¶œë ¥ ë° ì €ì¥
    print("ğŸ“Œ ì§ˆë¬¸:", question)
    print("ğŸ’¡ ë‹µë³€:", answer)
    save_answer_as_markdown(question, answer)
