import os
from dotenv import load_dotenv

# âœ… ìµœì‹  import ê²½ë¡œ ë°˜ì˜
from langchain_community.document_loaders import JSONLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Load API key
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()

# JSON íŒŒì¼ ê²½ë¡œ
JSON_FILE = "data/[ë¼ë²¨]ìˆ˜í•™ ì§€ì‹ì²´ê³„ ë°ì´í„° ì„¸íŠ¸_210611.json"
CHROMA_DB_DIR = "db/chroma_math_json"

# 1. JSON ë¬¸ì„œ ë¡œë“œ (fromConcept / toConcept ê°ê° ë¬¸ì„œí™”)
# 1. JSON ë¬¸ì„œ ë¡œë“œ (fromConcept / toConcept ê°ê° ë¬¸ì„œí™”)
def load_json(path):
    import json
    from langchain.schema import Document

    def safe_str(value):
        if value is None:
            return ""
        return str(value)

    with open(path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    documents = []

    for concept_id, concept_data in raw_data.items():
        for key in ["fromConcept", "toConcept"]:
            concept = concept_data.get(key, {})
            if not concept:
                continue

            content = f"{concept.get('name')}\n\n{concept.get('description', '')}"
            metadata = {
                "concept_id": safe_str(concept.get("id")),
                "semester": safe_str(concept.get("semester")),
                "chapter_id": safe_str(concept.get("chapter", {}).get("id")),
                "chapter_name": safe_str(concept.get("chapter", {}).get("name")),
                "achievement_id": safe_str(concept.get("achievement", {}).get("id")),
                "achievement_name": safe_str(concept.get("achievement", {}).get("name")),
                "relation_type": key,
                "parent_id": safe_str(concept_id)
            }

            documents.append(Document(page_content=content, metadata=metadata))

    return documents



# 2. ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì €ì¥
def save_to_vector_db(docs, persist_dir=CHROMA_DB_DIR):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = Chroma.from_documents(docs, embedding=embeddings, persist_directory=persist_dir)
    vectordb.persist()
    return vectordb

# 3. ì €ì¥ëœ ë²¡í„° DB ë¡œë“œ
def load_vector_db(persist_dir=CHROMA_DB_DIR):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    return vectordb

# 4. ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°
def ask_question_by_id(vectordb, concept_id, question):
    retriever = vectordb.as_retriever(
        search_kwargs={
            "k": 1,
            "filter": {
                "concept_id": str(concept_id)
            }
        }
    )

    # âœ… ë¬¸ìì—´ ì•ˆì—ì„œ concept_idë§Œ ì§ì ‘ ì‚½ì…
    prompt_template = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ìˆ˜í•™ êµê³¼ ê³¼ì •ì„ ì˜ ì•„ëŠ” êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì„œëŠ” idê°€ {concept_id}ì¸ ê°œë…ì— ëŒ€í•œ ì„¤ëª…, í•™ê¸° ì •ë³´, ê´€ë ¨ ë‹¨ì›, ì„±ì·¨ ê¸°ì¤€ ë“±ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

í•™ìƒì´ ì´ ê°œë…ì„ ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬:
1. ê°œë…ì„ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê³ ,
2. ì´ ê°œë…ì´ í¬í•¨ëœ ë‹¨ì›ê³¼ ëª©ì°¨ íë¦„ ì†ì—ì„œ ì–´ë–¤ ìœ„ì¹˜ì¸ì§€ ì†Œê°œí•˜ë©°,
3. ì„±ì·¨ ê¸°ì¤€ì´ ì˜ë¯¸í•˜ëŠ” ë‚´ìš©ì„ í’€ì–´ ì„¤ëª…í•˜ê³ ,
4. í•™ìƒì´ ì—°ìŠµí•´ë³¼ ìˆ˜ ìˆëŠ” ì‹¤ìƒí™œ ê¸°ë°˜ì˜ ë¬¸ì œë¥¼ í•œë‘ ê°œ ì œì‹œí•˜ê³ ,
5. LaTeX ìˆ˜ì‹ì´ í¬í•¨ëœ í•´ì„¤ì„ ì œê³µí•©ë‹ˆë‹¤.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”:

### ğŸ§  ê°œë… ì„¤ëª…
...

### ğŸ“˜ ë‹¨ì›ê³¼ ëª©ì°¨ ì •ë³´
...

### ğŸ¯ ì„±ì·¨ ê¸°ì¤€ í•´ì„¤
...

### ğŸ§© ì—°ìŠµ ë¬¸ì œ
ë¬¸ì œ 1: ...
ë¬¸ì œ 2: ...

### âœ… ì •ë‹µ ë° í•´ì„¤
ì •ë‹µ 1: ...
ì •ë‹µ 2: ...
"""


    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(
            model_name="gpt-3.5-turbo-16k",
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY
        ),
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": question})
    return result["result"]



# 5. ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥
def save_answer_as_markdown(question, answer, file_path="result.md"):
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(f"# ğŸ“Œ ì§ˆë¬¸\n\n{question}\n\n")
        f.write(f"# ğŸ’¡ ë‹µë³€\n\n{answer}\n")
    print(f"ğŸ“„ ë‹µë³€ì´ '{file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    # Step 1: JSON ë¡œë“œ
    documents = load_json(JSON_FILE)
    print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ì¡°ê°ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # Step 2: ë²¡í„° DBì— ì €ì¥
    vectordb = save_to_vector_db(documents)
    print("âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ")

    # Step 3: ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
    concept_id = 5844
    question = "í•™ìƒì´ ê°œë… id 5844ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì‰½ê²Œ ì„¤ëª…í•´ì£¼ê³  ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
    answer = ask_question_by_id(vectordb, concept_id, question)



    # Step 4: ì¶œë ¥ ë° ì €ì¥
    print("ğŸ“Œ ì§ˆë¬¸:", question)
    print("ğŸ’¡ ë‹µë³€:", answer)
    save_answer_as_markdown(question, answer)
