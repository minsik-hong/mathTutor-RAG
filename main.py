# main.py
import os
from fastapi import FastAPI, Request
from pydantic import BaseModel
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI  # ìµœì‹  ë²„ì „ ì‚¬ìš©

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()
CHROMA_DB_DIR = "db/chroma_e3"

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="ìˆ˜í•™ RAG ì±—ë´‡")

# CORS ì„¤ì • (í•„ìš”ì‹œ Frontend ì—°ë™ ì‹œ ì‚¬ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œ ë‹¨ê³„ì—ì„œëŠ” í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸ“¦ Vector DB ë¡œë“œ
def load_vector_db():
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = Chroma(
        persist_directory=CHROMA_DB_DIR,
        embedding_function=embeddings,
        collection_name="elementary-math"
    )
    return vectordb

# ğŸ”„ ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜
def ask_question(vectordb, question: str):
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(question)  # ìµœì‹  ë°©ì‹ìœ¼ë¡œ êµì²´

    # ğŸ”¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "ë‹¹ì‹ ì€ ìˆ˜í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤. "
            "í•™ìƒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n"
            "ë°°ê²½ ì •ë³´:\n{context}\n\n"
            "ì§ˆë¬¸:\n{question}\n\n"
            "ë‹µë³€:"
        )
    )

    # ğŸ”¸ LLM ì„¤ì •
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0.3,
        openai_api_key=OPENAI_API_KEY
    )

    # ğŸ”¸ í”„ë¡¬í”„íŠ¸ ì²´ì¸ êµ¬ì„±
    chain = (
        {"context": lambda x: "\n".join(doc.page_content for doc in x["docs"]), "question": lambda x: x["question"]}
        | prompt_template
        | llm
        | StrOutputParser()
    )

    # ğŸ”¸ ì‹¤í–‰
    result = chain.invoke({"docs": docs, "question": question})
    return result

# ğŸš€ ì„œë²„ ì‹œì‘ ì‹œ ë²¡í„° DB ë¡œë“œ
vectordb = load_vector_db()

# âœ… ìš”ì²­ ëª¨ë¸ ì •ì˜
class QueryRequest(BaseModel):
    question: str

# ğŸŒ POST API ì—”ë“œí¬ì¸íŠ¸
@app.post("/ask")
async def ask(request: QueryRequest):
    question = request.question
    print(f"ğŸ“¥ ì§ˆë¬¸ ë°›ìŒ: {question}")
    answer = ask_question(vectordb, question)
    return {"question": question, "answer": answer}
