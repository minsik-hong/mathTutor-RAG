# main.py
import os
from fastapi import FastAPI, Request
from pydantic import BaseModel
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI  # ìµœì‹  ë²„ì „ ì‚¬ìš©
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()
CHROMA_DB_DIR = "db/chroma_math_all"

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="ìˆ˜í•™ RAG ì±—ë´‡")

# CORS ì„¤ì • (í•„ìš”ì‹œ Frontend ì—°ë™ ì‹œ ì‚¬ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œ ë‹¨ê³„ì—ì„œëŠ” í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸ“¦ Vector DB ë¡œë“œ
def load_vector_db():
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = Chroma(
        persist_directory=CHROMA_DB_DIR,
        embedding_function=embeddings,
        collection_name="elementary-math"
    )
    return vectordb


# ğŸ”„ ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜ (Memory í¬í•¨)
def ask_question(vectordb, question: str):
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(question)

    # ğŸ”¹ ì´ì „ ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
    history = memory.chat_memory.messages  # List[HumanMessage | AIMessage]

    # ğŸ”¹ ëŒ€í™” ë‚´ì—­ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    history_text = ""
    for msg in history:
        role = "í•™ìƒ" if msg.type == "human" else "ì„ ìƒë‹˜"
        history_text += f"{role}: {msg.content}\n"

    # ğŸ”¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_template = PromptTemplate(
        input_variables=["history", "context", "question"],
        template=(
            "ë‹¹ì‹ ì€ ìˆ˜í•™ ì„ ìƒë‹˜ì…ë‹ˆë‹¤. í•™ìƒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n"
            "ë‹¤ìŒì€ ìˆ˜í•™ êµê³¼ì„œì—ì„œ ë°œì·Œí•œ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. \n\n"
            "ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ ë‹µí•  ë•ŒëŠ” ì´ˆ3~ì¤‘3 ìˆ˜ì¤€ì˜ ìˆ˜í•™ ë²”ìœ„ ë‚´ìš©ì„ ì•Œê³  ìˆë‹¤ê³  ëŒ€ë‹µí•´.\n\n"
            "ëª¨ë“  ë‹µë³€ì—ëŠ” ë‹µë³€ì˜ ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì„¸ìš”.\n\n"
            "ğŸ’¬ ì´ì „ ëŒ€í™”:\n{history}\n\n"
            "ğŸ“˜ êµê³¼ì„œ ë‚´ìš©:\n{context}\n\n"
            "ğŸ™‹â€â™€ï¸ ì§ˆë¬¸:\n{question}\n\n"
            "ğŸ§  ë‹µë³€:"
        )
    )

    # ğŸ”¸ LLM êµ¬ì„±
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0.3,
        openai_api_key=OPENAI_API_KEY
    )

    # ğŸ”¸ ì²´ì¸ êµ¬ì„± (context + ëŒ€í™” + ì§ˆë¬¸ ì „ë‹¬)
    chain = (
        {
            "context": lambda x: "\n".join(doc.page_content for doc in x["docs"]),
            "question": lambda x: x["question"],
            "history": lambda x: x["history"]
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )

    # ğŸ”¸ ì‹¤í–‰ + memory ì—…ë°ì´íŠ¸
    result = chain.invoke({"docs": docs, "question": question, "history": history_text})
    memory.chat_memory.add_user_message(question)
    memory.chat_memory.add_ai_message(result)

    return result


# ğŸš€ ì„œë²„ ì‹œì‘ ì‹œ ë²¡í„° DB ë¡œë“œ
vectordb = load_vector_db()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)



# âœ… ìš”ì²­ ëª¨ë¸ ì •ì˜
class QueryRequest(BaseModel):
    question: str

# ğŸŒ POST API ì—”ë“œí¬ì¸íŠ¸
@app.post("/ask")
async def ask(request: QueryRequest):
    question = request.question
    print(f"ğŸ“¥ ì§ˆë¬¸ ë°›ìŒ: {question}")

    answer = ask_question(vectordb, question)
    return {"question": question, "answer": answer}